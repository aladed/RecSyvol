import polars as pl
import gc
import logging
from pathlib import Path
from datetime import timedelta


logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
logger = logging.getLogger(__name__)

 
RAW_DIR = Path("dataset/full")
PROCESSED_DIR = Path("dataset/processed")
VOCAB_PATH = PROCESSED_DIR / "vocab.parquet"
OUTPUT_DIR = PROCESSED_DIR / "shards"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


NUM_SHARDS = 50 


TEST_DAYS_CUTOFF = 2 

def load_vocab_map():
    """
    Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð² Ð¿Ð°Ð¼ÑÑ‚ÑŒ ÐºÐ°Ðº HashMap (Dict) Ð´Ð»Ñ Ð¼Ð³Ð½Ð¾Ð²ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°.
    DataFrame -> Dict { 'MP_item_1': 543, ... }
    """
    logger.info("ðŸ“– Loading Vocabulary...")
    vocab_df = pl.read_parquet(VOCAB_PATH)

    return dict(zip(vocab_df["token_str"], vocab_df["token_id"]))

def get_domain_plan(domain_folder: Path, domain_prefix: str, vocab_map: dict):
    """
    Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð»ÐµÐ½Ð¸Ð²Ñ‹Ð¹ Ð¿Ð»Ð°Ð½ Ñ‡Ñ‚ÐµÐ½Ð¸Ñ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ð´Ð¾Ð¼ÐµÐ½Ð°.
    ÐŸÑ€ÐµÐ²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ item_id -> token_id.
    """

    file_paths = list((domain_folder / "events").glob("*.pq"))
    if not file_paths:
        return None


    q = pl.scan_parquet(file_paths)


    entity_col = "brand_id" if "reviews" in str(domain_folder) else "item_id"
    

    q = q.select([
        pl.col("user_id"),
        pl.col("timestamp"),
        pl.col(entity_col).cast(pl.Utf8) 
    ])


    q = q.with_columns(
        (pl.lit(domain_prefix) + pl.col(entity_col)).alias("token_key")
    )

    q = q.select([
        pl.col("user_id"),
        pl.col("timestamp"),
        pl.col("token_key").replace(vocab_map, default=4).cast(pl.UInt32).alias("token_id") 

    ])

    return q

def process_shards():
    """
    Ð“Ð»Ð°Ð²Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸.
    """
    
    vocab_map = load_vocab_map()
    

    domains = {
        RAW_DIR / "marketplace": "MP_",
        RAW_DIR / "retail": "RT_",
        RAW_DIR / "offers": "OF_",
        RAW_DIR / "reviews": "BR_" 
    }

    for shard_id in range(NUM_SHARDS):
        logger.info(f"ðŸ”¨ Processing Shard {shard_id + 1}/{NUM_SHARDS}...")

        plans = []
        for domain_path, prefix in domains.items():
            lazy_df = get_domain_plan(domain_path, prefix, vocab_map)
            
            if lazy_df is not None:
                sharded_df = lazy_df.filter(
                    (pl.col("user_id").hash() % NUM_SHARDS) == shard_id
                )
                plans.append(sharded_df)

        if not plans:
            continue

        combined_lazy = pl.concat(plans)

        df_shard = combined_lazy.collect()

        if df_shard.height == 0:
            continue

        df_shard = df_shard.sort(["user_id", "timestamp"])

        sequences = df_shard.group_by("user_id").agg([
            pl.col("token_id").alias("sequence"),
            pl.col("timestamp").alias("timestamps") 
        ])

        max_time = df_shard["timestamp"].max()
        cutoff_time = max_time - timedelta(days=TEST_DAYS_CUTOFF)

        
        output_file = OUTPUT_DIR / f"shard_{shard_id}.parquet"
        sequences.write_parquet(output_file)
        
        logger.info(f"âœ… Saved shard {shard_id} to {output_file} (Users: {sequences.height})")

        del df_shard
        del sequences
        gc.collect()

if __name__ == "__main__":
    process_shards()